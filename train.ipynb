{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gen_model import Generator\n",
    "from disc_model import Discriminator\n",
    "from utils import *\n",
    "from step_assert import AssertStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = [256, 256, 128, 64, 64]\n",
    "betas = (0.0, 0.99)\n",
    "noise_dim = 256\n",
    "step = 1\n",
    "max_steps = 4\n",
    "res_list = [4, 8, 16, 32, 64]\n",
    "fade_size = 800_000\n",
    "lr = 0.005\n",
    "phase_size = [800_000, 1_600_000, 2_000_000, 2_500_000, 3_500_000]\n",
    "GRAD_VAL = 10\n",
    "disc_train_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definitions\n",
    "generator = Generator(noise_dim).to(device)\n",
    "generator.apply(weights_init)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Generator parameters: 2.229.392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (tanh): Tanh()\n",
       "  (inp): GenBlock(\n",
       "    (conv1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky1): LeakyReLU(negative_slope=0.2)\n",
       "    (conv2): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv1): GenBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky1): LeakyReLU(negative_slope=0.2)\n",
       "    (conv2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv2): GenBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky1): LeakyReLU(negative_slope=0.2)\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv3): GenBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky1): LeakyReLU(negative_slope=0.2)\n",
       "    (conv2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv4): GenBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky1): LeakyReLU(negative_slope=0.2)\n",
       "    (conv2): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (leaky2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (torgb): ModuleList(\n",
       "    (0): ConvTranspose2d(256, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): ConvTranspose2d(128, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (2): ConvTranspose2d(64, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (3): ConvTranspose2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): ConvTranspose2d(16, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pr = large_num_period(count_parameters(generator))\n",
    "print(f\"Trainable Generator parameters: {g_pr}\")\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Discriminator parameters: 2.229.968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): DiscBlock(\n",
       "    (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (conv2): DiscBlock(\n",
       "    (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (conv3): DiscBlock(\n",
       "    (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (conv4): DiscBlock(\n",
       "    (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (outp): DecisionBlock(\n",
       "    (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    (conv): Conv2d(257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (pool): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (flatten): Flatten()\n",
       "    (outp): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       "  (fromrgb): ModuleList(\n",
       "    (0): InpBlock(\n",
       "      (inp): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): InpBlock(\n",
       "      (inp): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): InpBlock(\n",
       "      (inp): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): InpBlock(\n",
       "      (inp): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): InpBlock(\n",
       "      (inp): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pr = large_num_period(count_parameters(discriminator))\n",
    "print(f\"Trainable Discriminator parameters: {d_pr}\")\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_noise = torch.randn(64, noise_dim).cuda()\n",
    "def generate_and_save_images(iteration):\n",
    "    fake_folder = r\"C:\\Users\\Johnny\\Desktop\\PROGAN\\intermediate_images\"\n",
    "    fake_img_path = os.path.join(fake_folder, f\"iteration{iteration}resolution{res_list[step]}x{res_list[step]}\")\n",
    "    with torch.no_grad():\n",
    "        images = generator(preview_noise, step=step, alpha=alpha).detach().cpu()\n",
    "        images = np.transpose(vutils.make_grid(images, padding=2, normalize=True), (1,2,0))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    plt.imshow(images)\n",
    "    plt.savefig(fake_img_path)\n",
    "    plt.close()\n",
    "    \n",
    "def generate_final_images(num=128):\n",
    "    image_folder = r\"C:\\Users\\Johnny\\Desktop\\PROGAN\\final_images\"\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num, noise_dim).cuda()\n",
    "        images = generator(noise, step=step, alpha=alpha).detach().cpu()\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i]\n",
    "        save_image(image,\n",
    "                   os.path.join(image_folder, f\"image{i}.jpg\"),\n",
    "                   normalize=True,\n",
    "                   range=(-1, 1))\n",
    "        \n",
    "val_noise = torch.randn(256, noise_dim).cuda()\n",
    "def calcMFID(real_images):\n",
    "    with torch.no_grad():\n",
    "        real_predict = discriminator(real_images, step=step, alpha=alpha).mean()\n",
    "        val_images = generator(val_noise, step=step, alpha=alpha)\n",
    "        val_predict = discriminator(val_images, step=step, alpha=alpha).mean()\n",
    "        MFID_score = real_predict - val_predict\n",
    "    return MFID_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=10_000_000):\n",
    "    global step, alpha\n",
    "    early_stopper = AssertStep(tolerance=4, buffer=2)\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    data = new_dataloader(batch_size[step], res_list[step])\n",
    "    used_samples = 0\n",
    "    start = time.time()\n",
    "    print(\"Starting Training Loop...\")\n",
    "    mfid_batch = next(iter(data))[0].cuda().float()\n",
    "    step_start = time.time()\n",
    "    for current_iteration in range(iterations):\n",
    "            if used_samples > phase_size[step]:\n",
    "                torch.save(generator, \"generator.pt\")\n",
    "                torch.save(discriminator, \"discriminator.pt\")\n",
    "                step_time_taken = (time.time() - step_start) // 60\n",
    "                step_start = time.time()\n",
    "                print(f\"Time taken for resolution {res_list[step]}x{res_list[step]} is {step_time_taken} minutes, Used Samples: {used_samples}, loss_MFID: {calcMFID(mfid_batch)}\")\n",
    "                print()\n",
    "                generate_and_save_images(current_iteration)\n",
    "                samples = large_num_period(used_samples)\n",
    "                used_samples = 0\n",
    "                step += 1\n",
    "                if step > max_steps:\n",
    "                    step = max_steps \n",
    "                    break\n",
    "                adjust_lr(d_optimizer, lr)\n",
    "                adjust_lr(g_optimizer, lr)\n",
    "                data = new_dataloader(batch_size[step], res_list[step])\n",
    "                mfid_batch = next(iter(data))[0].cuda().float()\n",
    "                loader = iter(data)\n",
    "                \n",
    "            alpha = min([1, (used_samples + 1) /  fade_size]) if step > 1 else 1\n",
    "            try:\n",
    "                batch = next(loader)\n",
    "            except (NameError, StopIteration):\n",
    "                loader = iter(data)\n",
    "                batch = next(loader)\n",
    "\n",
    "            real_images = batch[0].cuda().float()\n",
    "            current_bs = real_images.shape[0]\n",
    "            real_predict = discriminator(real_images, step=step, alpha=alpha).mean()\n",
    "\n",
    "            # random noise vector sampling values of gaussian distribution\n",
    "            gen_noise = torch.randn(current_bs, noise_dim).cuda()\n",
    "            gen_imgs = generator(gen_noise, step=step, alpha=alpha)\n",
    "\n",
    "            fake_predict = discriminator(gen_imgs, step=step, alpha=alpha).mean()\n",
    "\n",
    "            # wgan-gp loss for discriminator - maximize (d(r) - d(f)) -> wasserstein distance\n",
    "            gp = gradient_penalty(discriminator, real_images, gen_imgs, step, device=device, alpha=alpha)\n",
    "            disc_loss = -(real_predict - fake_predict) + (GRAD_VAL * gp)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            if current_iteration % disc_train_count == 0:\n",
    "                disc_loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                disc_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            if current_iteration % disc_train_count == 0:\n",
    "                used_samples += current_bs\n",
    "                # do another forward pass on fake images\n",
    "                gen_predict = discriminator(gen_imgs, step=step, alpha=alpha).mean()\n",
    "                # g loss - maximize d(f)\n",
    "                gen_loss = -gen_predict\n",
    "                generator.zero_grad()\n",
    "                gen_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "            if current_iteration % 5_000 == 0:\n",
    "                mfid = calcMFID(mfid_batch)\n",
    "                \n",
    "                    \n",
    "                g_losses.append((real_predict - fake_predict).detach().item())\n",
    "                d_losses.append(disc_loss.detach().item())\n",
    "                generate_and_save_images(current_iteration)\n",
    "                plot_losses(g_losses, d_losses)\n",
    "                samples = large_num_period(used_samples)\n",
    "                iter_nr = large_num_period(current_iteration)\n",
    "                print(f\"[{iter_nr}] Resolution: {res_list[step]}x{res_list[step]}, loss_MFID: {mfid}, Samples: {samples}, alpha: {alpha}, Time: {(time.time()-start) // 60} minutes\")\n",
    "    print(f\"Training took {(time.time()-start) // 60} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0] Resolution: 8x8, loss_MFID: -6.335700988769531, Samples: 256, alpha: 1, Time: 0.0 minutes\n",
      "[5.000] Resolution: 8x8, loss_MFID: -0.6701259613037109, Samples: 256.103, alpha: 1, Time: 12.0 minutes\n",
      "[10.000] Resolution: 8x8, loss_MFID: -0.18227386474609375, Samples: 511.950, alpha: 1, Time: 26.0 minutes\n",
      "[15.000] Resolution: 8x8, loss_MFID: 0.06898641586303711, Samples: 767.644, alpha: 1, Time: 39.0 minutes\n",
      "[20.000] Resolution: 8x8, loss_MFID: 0.05788230895996094, Samples: 1.023.491, alpha: 1, Time: 52.0 minutes\n",
      "[25.000] Resolution: 8x8, loss_MFID: -0.19026947021484375, Samples: 1.279.338, alpha: 1, Time: 66.0 minutes\n",
      "[30.000] Resolution: 8x8, loss_MFID: 0.04847431182861328, Samples: 1.535.185, alpha: 1, Time: 79.0 minutes\n",
      "Time taken for resolution 8x8 is 82.0 minutes, Used Samples: 1600056, loss_MFID: 0.005260467529296875\n",
      "\n",
      "[35.000] Resolution: 16x16, loss_MFID: 1.9728317260742188, Samples: 95.488, alpha: 0.11920125, Time: 91.0 minutes\n",
      "[40.000] Resolution: 16x16, loss_MFID: 1.212005615234375, Samples: 223.463, alpha: 0.27917, Time: 102.0 minutes\n",
      "[45.000] Resolution: 16x16, loss_MFID: 0.8458404541015625, Samples: 351.463, alpha: 0.43917, Time: 114.0 minutes\n",
      "[50.000] Resolution: 16x16, loss_MFID: 0.605316162109375, Samples: 479.438, alpha: 0.59913875, Time: 126.0 minutes\n",
      "[55.000] Resolution: 16x16, loss_MFID: 0.6534576416015625, Samples: 607.438, alpha: 0.75913875, Time: 137.0 minutes\n",
      "[60.000] Resolution: 16x16, loss_MFID: 0.545379638671875, Samples: 735.413, alpha: 0.9191075, Time: 149.0 minutes\n",
      "[65.000] Resolution: 16x16, loss_MFID: 0.44481658935546875, Samples: 863.388, alpha: 1, Time: 161.0 minutes\n",
      "[70.000] Resolution: 16x16, loss_MFID: 0.44904327392578125, Samples: 991.388, alpha: 1, Time: 172.0 minutes\n",
      "[75.000] Resolution: 16x16, loss_MFID: 0.4223785400390625, Samples: 1.119.363, alpha: 1, Time: 183.0 minutes\n",
      "[80.000] Resolution: 16x16, loss_MFID: 0.3877716064453125, Samples: 1.247.338, alpha: 1, Time: 194.0 minutes\n",
      "[85.000] Resolution: 16x16, loss_MFID: 0.37021636962890625, Samples: 1.375.338, alpha: 1, Time: 206.0 minutes\n",
      "[90.000] Resolution: 16x16, loss_MFID: 0.3076362609863281, Samples: 1.503.313, alpha: 1, Time: 217.0 minutes\n",
      "[95.000] Resolution: 16x16, loss_MFID: 0.35407257080078125, Samples: 1.631.288, alpha: 1, Time: 228.0 minutes\n",
      "[100.000] Resolution: 16x16, loss_MFID: 0.32776641845703125, Samples: 1.759.288, alpha: 1, Time: 240.0 minutes\n",
      "[105.000] Resolution: 16x16, loss_MFID: 0.36382293701171875, Samples: 1.887.263, alpha: 1, Time: 251.0 minutes\n",
      "Time taken for resolution 16x16 is 179.0 minutes, Used Samples: 2000031, loss_MFID: 0.35628509521484375\n",
      "\n",
      "[110.000] Resolution: 32x32, loss_MFID: 1.581878662109375, Samples: 7.616, alpha: 0.00944125, Time: 263.0 minutes\n",
      "[115.000] Resolution: 32x32, loss_MFID: 20.166183471679688, Samples: 71.616, alpha: 0.08944125, Time: 273.0 minutes\n",
      "[120.000] Resolution: 32x32, loss_MFID: 7.9309844970703125, Samples: 135.616, alpha: 0.16944125, Time: 284.0 minutes\n",
      "[125.000] Resolution: 32x32, loss_MFID: 2.3590087890625, Samples: 199.616, alpha: 0.24944125, Time: 295.0 minutes\n",
      "[130.000] Resolution: 32x32, loss_MFID: 2.806610107421875, Samples: 263.591, alpha: 0.32941, Time: 306.0 minutes\n",
      "[135.000] Resolution: 32x32, loss_MFID: 1.576202392578125, Samples: 327.591, alpha: 0.40941, Time: 317.0 minutes\n",
      "[140.000] Resolution: 32x32, loss_MFID: 2.169952392578125, Samples: 391.591, alpha: 0.48941, Time: 328.0 minutes\n",
      "[145.000] Resolution: 32x32, loss_MFID: 1.08599853515625, Samples: 455.566, alpha: 0.56937875, Time: 339.0 minutes\n",
      "[150.000] Resolution: 32x32, loss_MFID: 1.207794189453125, Samples: 519.566, alpha: 0.64937875, Time: 350.0 minutes\n",
      "[155.000] Resolution: 32x32, loss_MFID: 0.65936279296875, Samples: 583.566, alpha: 0.72937875, Time: 361.0 minutes\n",
      "[160.000] Resolution: 32x32, loss_MFID: 1.36407470703125, Samples: 647.541, alpha: 0.8093475, Time: 371.0 minutes\n",
      "[165.000] Resolution: 32x32, loss_MFID: 1.0846099853515625, Samples: 711.541, alpha: 0.8893475, Time: 382.0 minutes\n",
      "[170.000] Resolution: 32x32, loss_MFID: 0.4667205810546875, Samples: 775.541, alpha: 0.9693475, Time: 392.0 minutes\n",
      "[175.000] Resolution: 32x32, loss_MFID: 0.86883544921875, Samples: 839.516, alpha: 1, Time: 402.0 minutes\n",
      "[180.000] Resolution: 32x32, loss_MFID: 0.72503662109375, Samples: 903.516, alpha: 1, Time: 413.0 minutes\n",
      "[185.000] Resolution: 32x32, loss_MFID: 1.17926025390625, Samples: 967.516, alpha: 1, Time: 423.0 minutes\n",
      "[190.000] Resolution: 32x32, loss_MFID: 1.3738861083984375, Samples: 1.031.491, alpha: 1, Time: 433.0 minutes\n",
      "[195.000] Resolution: 32x32, loss_MFID: 0.85882568359375, Samples: 1.095.491, alpha: 1, Time: 443.0 minutes\n",
      "[200.000] Resolution: 32x32, loss_MFID: 0.685821533203125, Samples: 1.159.491, alpha: 1, Time: 453.0 minutes\n",
      "[205.000] Resolution: 32x32, loss_MFID: 1.083587646484375, Samples: 1.223.466, alpha: 1, Time: 463.0 minutes\n",
      "[210.000] Resolution: 32x32, loss_MFID: 0.8063812255859375, Samples: 1.287.466, alpha: 1, Time: 473.0 minutes\n",
      "[215.000] Resolution: 32x32, loss_MFID: 1.967193603515625, Samples: 1.351.466, alpha: 1, Time: 483.0 minutes\n",
      "[220.000] Resolution: 32x32, loss_MFID: 1.277008056640625, Samples: 1.415.466, alpha: 1, Time: 493.0 minutes\n",
      "[225.000] Resolution: 32x32, loss_MFID: 2.0870208740234375, Samples: 1.479.441, alpha: 1, Time: 503.0 minutes\n",
      "[230.000] Resolution: 32x32, loss_MFID: 1.1224136352539062, Samples: 1.543.441, alpha: 1, Time: 513.0 minutes\n",
      "[235.000] Resolution: 32x32, loss_MFID: 0.8739776611328125, Samples: 1.607.441, alpha: 1, Time: 523.0 minutes\n",
      "[240.000] Resolution: 32x32, loss_MFID: 0.9660491943359375, Samples: 1.671.416, alpha: 1, Time: 533.0 minutes\n",
      "[245.000] Resolution: 32x32, loss_MFID: 1.0170745849609375, Samples: 1.735.416, alpha: 1, Time: 543.0 minutes\n",
      "[250.000] Resolution: 32x32, loss_MFID: 1.6504974365234375, Samples: 1.799.416, alpha: 1, Time: 553.0 minutes\n",
      "[255.000] Resolution: 32x32, loss_MFID: 1.1614990234375, Samples: 1.863.391, alpha: 1, Time: 563.0 minutes\n",
      "[260.000] Resolution: 32x32, loss_MFID: 0.8280410766601562, Samples: 1.927.391, alpha: 1, Time: 572.0 minutes\n",
      "[265.000] Resolution: 32x32, loss_MFID: 1.0823287963867188, Samples: 1.991.391, alpha: 1, Time: 582.0 minutes\n",
      "[270.000] Resolution: 32x32, loss_MFID: 0.8232040405273438, Samples: 2.055.366, alpha: 1, Time: 592.0 minutes\n",
      "[275.000] Resolution: 32x32, loss_MFID: 1.3201370239257812, Samples: 2.119.366, alpha: 1, Time: 602.0 minutes\n",
      "[280.000] Resolution: 32x32, loss_MFID: 1.9385604858398438, Samples: 2.183.366, alpha: 1, Time: 612.0 minutes\n",
      "[285.000] Resolution: 32x32, loss_MFID: 1.2176055908203125, Samples: 2.247.341, alpha: 1, Time: 621.0 minutes\n",
      "[290.000] Resolution: 32x32, loss_MFID: 0.9721221923828125, Samples: 2.311.341, alpha: 1, Time: 631.0 minutes\n",
      "[295.000] Resolution: 32x32, loss_MFID: 0.26981353759765625, Samples: 2.375.341, alpha: 1, Time: 641.0 minutes\n",
      "[300.000] Resolution: 32x32, loss_MFID: 1.0523452758789062, Samples: 2.439.316, alpha: 1, Time: 651.0 minutes\n",
      "Time taken for resolution 32x32 is 399.0 minutes, Used Samples: 2500052, loss_MFID: 0.5461044311523438\n",
      "\n",
      "[305.000] Resolution: 64x64, loss_MFID: 1.8061676025390625, Samples: 3.264, alpha: 0.00400125, Time: 662.0 minutes\n",
      "[310.000] Resolution: 64x64, loss_MFID: 11.01190185546875, Samples: 67.264, alpha: 0.08400125, Time: 682.0 minutes\n",
      "[315.000] Resolution: 64x64, loss_MFID: 1.210479736328125, Samples: 131.264, alpha: 0.16400125, Time: 701.0 minutes\n",
      "[320.000] Resolution: 64x64, loss_MFID: 0.1892242431640625, Samples: 195.264, alpha: 0.24400125, Time: 720.0 minutes\n",
      "[325.000] Resolution: 64x64, loss_MFID: 4.2938690185546875, Samples: 259.239, alpha: 0.32397, Time: 739.0 minutes\n",
      "[330.000] Resolution: 64x64, loss_MFID: 2.1529541015625, Samples: 323.239, alpha: 0.40397, Time: 758.0 minutes\n",
      "[335.000] Resolution: 64x64, loss_MFID: 2.1013336181640625, Samples: 387.239, alpha: 0.48397, Time: 777.0 minutes\n",
      "[340.000] Resolution: 64x64, loss_MFID: 1.142669677734375, Samples: 451.214, alpha: 0.56393875, Time: 796.0 minutes\n",
      "[345.000] Resolution: 64x64, loss_MFID: 2.3596038818359375, Samples: 515.214, alpha: 0.64393875, Time: 815.0 minutes\n",
      "[350.000] Resolution: 64x64, loss_MFID: 1.3977508544921875, Samples: 579.214, alpha: 0.72393875, Time: 834.0 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355.000] Resolution: 64x64, loss_MFID: 1.4723968505859375, Samples: 643.189, alpha: 0.8039075, Time: 853.0 minutes\n",
      "[360.000] Resolution: 64x64, loss_MFID: 0.949920654296875, Samples: 707.189, alpha: 0.8839075, Time: 872.0 minutes\n",
      "[365.000] Resolution: 64x64, loss_MFID: 1.0304718017578125, Samples: 771.189, alpha: 0.9639075, Time: 891.0 minutes\n",
      "[370.000] Resolution: 64x64, loss_MFID: 1.9460296630859375, Samples: 835.164, alpha: 1, Time: 909.0 minutes\n",
      "[375.000] Resolution: 64x64, loss_MFID: 1.6615447998046875, Samples: 899.164, alpha: 1, Time: 927.0 minutes\n",
      "[380.000] Resolution: 64x64, loss_MFID: 1.8035888671875, Samples: 963.164, alpha: 1, Time: 945.0 minutes\n",
      "[385.000] Resolution: 64x64, loss_MFID: 2.1310272216796875, Samples: 1.027.139, alpha: 1, Time: 963.0 minutes\n",
      "[390.000] Resolution: 64x64, loss_MFID: 1.4591064453125, Samples: 1.091.139, alpha: 1, Time: 981.0 minutes\n",
      "[395.000] Resolution: 64x64, loss_MFID: 1.471527099609375, Samples: 1.155.139, alpha: 1, Time: 999.0 minutes\n",
      "[400.000] Resolution: 64x64, loss_MFID: 1.862213134765625, Samples: 1.219.114, alpha: 1, Time: 1016.0 minutes\n",
      "[405.000] Resolution: 64x64, loss_MFID: 1.333251953125, Samples: 1.283.114, alpha: 1, Time: 1034.0 minutes\n",
      "[410.000] Resolution: 64x64, loss_MFID: 1.7333526611328125, Samples: 1.347.114, alpha: 1, Time: 1052.0 minutes\n",
      "[415.000] Resolution: 64x64, loss_MFID: 1.5113525390625, Samples: 1.411.114, alpha: 1, Time: 1070.0 minutes\n",
      "[420.000] Resolution: 64x64, loss_MFID: 2.1065673828125, Samples: 1.475.089, alpha: 1, Time: 1088.0 minutes\n",
      "[425.000] Resolution: 64x64, loss_MFID: 1.6159210205078125, Samples: 1.539.089, alpha: 1, Time: 1106.0 minutes\n",
      "[430.000] Resolution: 64x64, loss_MFID: 2.3399200439453125, Samples: 1.603.089, alpha: 1, Time: 1124.0 minutes\n",
      "[435.000] Resolution: 64x64, loss_MFID: 1.8189239501953125, Samples: 1.667.064, alpha: 1, Time: 1142.0 minutes\n",
      "[440.000] Resolution: 64x64, loss_MFID: 1.7471466064453125, Samples: 1.731.064, alpha: 1, Time: 1160.0 minutes\n",
      "[445.000] Resolution: 64x64, loss_MFID: 1.4074249267578125, Samples: 1.795.064, alpha: 1, Time: 1178.0 minutes\n",
      "[450.000] Resolution: 64x64, loss_MFID: 0.8090667724609375, Samples: 1.859.039, alpha: 1, Time: 1196.0 minutes\n",
      "[455.000] Resolution: 64x64, loss_MFID: 1.872528076171875, Samples: 1.923.039, alpha: 1, Time: 1214.0 minutes\n",
      "[460.000] Resolution: 64x64, loss_MFID: 1.57366943359375, Samples: 1.987.039, alpha: 1, Time: 1232.0 minutes\n",
      "[465.000] Resolution: 64x64, loss_MFID: 1.910064697265625, Samples: 2.051.014, alpha: 1, Time: 1249.0 minutes\n",
      "[470.000] Resolution: 64x64, loss_MFID: 0.7770538330078125, Samples: 2.115.014, alpha: 1, Time: 1267.0 minutes\n",
      "[475.000] Resolution: 64x64, loss_MFID: 1.5679168701171875, Samples: 2.179.014, alpha: 1, Time: 1285.0 minutes\n",
      "[480.000] Resolution: 64x64, loss_MFID: 1.60186767578125, Samples: 2.242.989, alpha: 1, Time: 1303.0 minutes\n",
      "[485.000] Resolution: 64x64, loss_MFID: 2.2040863037109375, Samples: 2.306.989, alpha: 1, Time: 1321.0 minutes\n",
      "[490.000] Resolution: 64x64, loss_MFID: 2.3748321533203125, Samples: 2.370.989, alpha: 1, Time: 1339.0 minutes\n",
      "[495.000] Resolution: 64x64, loss_MFID: 0.7679901123046875, Samples: 2.434.964, alpha: 1, Time: 1357.0 minutes\n",
      "[500.000] Resolution: 64x64, loss_MFID: 2.588287353515625, Samples: 2.498.964, alpha: 1, Time: 1375.0 minutes\n",
      "[505.000] Resolution: 64x64, loss_MFID: 2.534912109375, Samples: 2.562.964, alpha: 1, Time: 1393.0 minutes\n",
      "[510.000] Resolution: 64x64, loss_MFID: 2.4230194091796875, Samples: 2.626.964, alpha: 1, Time: 1411.0 minutes\n",
      "[515.000] Resolution: 64x64, loss_MFID: 2.25390625, Samples: 2.690.939, alpha: 1, Time: 1429.0 minutes\n",
      "[520.000] Resolution: 64x64, loss_MFID: 2.467132568359375, Samples: 2.754.939, alpha: 1, Time: 1448.0 minutes\n",
      "[525.000] Resolution: 64x64, loss_MFID: 1.0616302490234375, Samples: 2.818.939, alpha: 1, Time: 1466.0 minutes\n",
      "[530.000] Resolution: 64x64, loss_MFID: 1.6500396728515625, Samples: 2.882.914, alpha: 1, Time: 1484.0 minutes\n",
      "[535.000] Resolution: 64x64, loss_MFID: 2.09759521484375, Samples: 2.946.914, alpha: 1, Time: 1502.0 minutes\n",
      "[540.000] Resolution: 64x64, loss_MFID: 2.520538330078125, Samples: 3.010.914, alpha: 1, Time: 1520.0 minutes\n",
      "[545.000] Resolution: 64x64, loss_MFID: 2.1298370361328125, Samples: 3.074.889, alpha: 1, Time: 1538.0 minutes\n",
      "[550.000] Resolution: 64x64, loss_MFID: 2.2886962890625, Samples: 3.138.889, alpha: 1, Time: 1556.0 minutes\n",
      "[555.000] Resolution: 64x64, loss_MFID: 1.4549407958984375, Samples: 3.202.889, alpha: 1, Time: 1573.0 minutes\n",
      "[560.000] Resolution: 64x64, loss_MFID: 1.696868896484375, Samples: 3.266.864, alpha: 1, Time: 1590.0 minutes\n",
      "[565.000] Resolution: 64x64, loss_MFID: 2.1258087158203125, Samples: 3.330.864, alpha: 1, Time: 1608.0 minutes\n",
      "[570.000] Resolution: 64x64, loss_MFID: 1.3262939453125, Samples: 3.394.864, alpha: 1, Time: 1625.0 minutes\n",
      "[575.000] Resolution: 64x64, loss_MFID: 1.246307373046875, Samples: 3.458.839, alpha: 1, Time: 1642.0 minutes\n",
      "Time taken for resolution 64x64 is 993.0 minutes, Used Samples: 3500055, loss_MFID: 2.3754730224609375\n",
      "\n",
      "Training took 1654.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "# save generator model\n",
    "torch.save(generator, \"generator.pt\")\n",
    "torch.save(discriminator, \"discriminator.pt\")\n",
    "# generate images\n",
    "generate_final_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_final_images(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ef4880925456>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_img_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgenerate_and_save_images2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-ef4880925456>\u001b[0m in \u001b[0;36mgenerate_and_save_images2\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpreview_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfake_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\Johnny\\Desktop\\PROGAN\\\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfake_img_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"iteration{iteration}resolution{res_list[step]}x{res_list[step]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreview_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iteration' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_and_save_images2():\n",
    "    preview_noise = torch.randn(64, noise_dim).cuda()\n",
    "    fake_folder = r\"C:\\Users\\Johnny\\Desktop\\PROGAN\\\"\"\n",
    "    fake_img_path = os.path.join(fake_folder, f\"resolution{res_list[step]}x{res_list[step]}\")\n",
    "    with torch.no_grad():\n",
    "        images = generator(preview_noise, step=step, alpha=alpha).detach().cpu()\n",
    "        images = np.transpose(vutils.make_grid(images, padding=2, normalize=True), (1,2,0))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    plt.imshow(images)\n",
    "    plt.savefig(fake_img_path)\n",
    "    plt.close()\n",
    "generate_and_save_images2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
